# coding=utf-8

# Generated by CodiumAI

import pytest
import pytorch_lightning as pl
import torch

from atommic.collections.common.callbacks import EMA
from atommic.collections.common.callbacks.ema import EMAOptimizer


class TestEMA:
    # Tests that the EMA callback is initialized with the specified decay, validate_original_weights, every_n_steps,
    # and cpu_offload parameters.
    def test_initialize_ema_callback(self):
        decay = 0.9
        validate_original_weights = True
        every_n_steps = 2
        cpu_offload = False

        ema_callback = EMA(decay, validate_original_weights, every_n_steps, cpu_offload)

        assert ema_callback.decay == decay
        assert ema_callback.validate_original_weights == validate_original_weights
        assert ema_callback.every_n_steps == every_n_steps
        assert ema_callback.cpu_offload == cpu_offload

    # Tests that the swap_model_weights method swaps the model weights with the EMA weights.
    def test_swap_model_weights(self):
        ema_callback = EMA(0.9)
        trainer = pl.Trainer()
        trainer.optimizers = [
            EMAOptimizer(
                torch.optim.SGD(torch.nn.Linear(10, 10).parameters(), lr=0.1),
                device=torch.device("cpu"),
                decay=0.9,
                every_n_steps=-1,
                current_step=trainer.global_step,
            ),
            EMAOptimizer(
                torch.optim.Adam(torch.nn.Linear(10, 10).parameters(), lr=0.1),
                device=torch.device("cpu"),
                decay=0.9,
                every_n_steps=-1,
                current_step=trainer.global_step,
            ),
        ]
        trainer.optimizers[0].param_groups[0]["params"][0].data = torch.tensor([1.0])
        trainer.optimizers[1].param_groups[0]["params"][0].data = torch.tensor([2.0])

        ema_callback.swap_model_weights(trainer)

        assert trainer.optimizers[0].param_groups[0]["params"][0].data == torch.tensor([1.0])
        assert trainer.optimizers[1].param_groups[0]["params"][0].data == torch.tensor([2.0])

    # Tests that the save_ema_model context manager swaps the model weights with the EMA weights and yields.
    def test_save_ema_model_context_manager(self):
        trainer = pl.Trainer()
        trainer.optimizers = [
            EMAOptimizer(
                torch.optim.SGD(torch.nn.Linear(10, 10).parameters(), lr=0.1),
                device=torch.device("cpu"),
                decay=0.9,
                every_n_steps=-1,
                current_step=trainer.global_step,
            )
        ]
        ema_callback = EMA(0.9)
        trainer.callbacks.append(ema_callback)

        trainer.optimizers[0].param_groups[0]["params"][0].data = torch.tensor([1.0])

        with ema_callback.save_ema_model(trainer):
            assert trainer.optimizers[0].param_groups[0]["params"][0].data == torch.tensor([1.0])

        assert trainer.optimizers[0].param_groups[0]["params"][0].data == torch.tensor([1.0])

    # Tests that the save_original_optimizer_state context manager sets save_original_optimizer_state to True for
    # each optimizer and yields.
    def test_save_original_optimizer_state_context_manager(self):
        ema_callback = EMA(0.9)
        trainer = pl.Trainer()
        trainer.optimizers = [
            EMAOptimizer(
                torch.optim.SGD(torch.nn.Linear(10, 10).parameters(), lr=0.1),
                device=torch.device("cpu"),
                decay=0.9,
                every_n_steps=-1,
                current_step=trainer.global_step,
            ),
            EMAOptimizer(
                torch.optim.Adam(torch.nn.Linear(10, 10).parameters(), lr=0.1),
                device=torch.device("cpu"),
                decay=0.9,
                every_n_steps=-1,
                current_step=trainer.global_step,
            ),
        ]

        with ema_callback.save_original_optimizer_state(trainer):
            assert trainer.optimizers[0].save_original_optimizer_state is True
            assert trainer.optimizers[1].save_original_optimizer_state is True

        assert trainer.optimizers[0].save_original_optimizer_state is False
        assert trainer.optimizers[1].save_original_optimizer_state is False
