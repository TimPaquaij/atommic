{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7LfkL2r2Q1tr"
   },
   "source": [
    "# Getting Started: ATOMMIC Fundamentals\n",
    "\n",
    "Advanced Toolbox for Multitask Medical Imaging Consistency (ATOMMIC), is a toolbox for applying AI methods for accelerated MRI reconstruction (REC), MRI segmentation (SEG), quantitative MR imaging (qMRI), as well as multitask learning (MTL), i.e. performing multiple tasks simultaneously, such as reconstruction and segmentation. \n",
    "\n",
    "Each task is implemented in a separate collection, which consists of data loaders, transformations, models, metrics, and losses. A\n",
    "\n",
    "ATOMMIC is designed to be modular and extensible, and it is easy to add new tasks, models, and datasets. \n",
    "\n",
    "ATOMMIC uses PyTorch Lightning for feasible high-performance multi-GPU/multi-node mixed-precision training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-04T21:31:08.301721Z",
     "start_time": "2023-10-04T21:31:01.734736Z"
    },
    "id": "zLSy94NEQi-e"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "You can run either this notebook locally (if you have all the dependencies and a GPU) or on Google Colab.\n",
    "\n",
    "Instructions for setting up Colab are as follows:\n",
    "1. Open a new Python 3 notebook.\n",
    "2. Import this notebook from GitHub (File -> Upload Notebook -> \"GITHUB\" tab -> copy/paste GitHub URL)\n",
    "3. Connect to an instance with a GPU (Runtime -> Change runtime type -> select \"GPU\" for hardware accelerator)\n",
    "4. Run this cell to set up dependencies.\n",
    "\"\"\"\n",
    "# If you're using Google Colab and not running locally, run this cell.\n",
    "\n",
    "## Install dependencies\n",
    "!pip install wget\n",
    "!apt-get install sox libsndfile1 ffmpeg\n",
    "!pip install text-unidecode\n",
    "\n",
    "# ## Install ATOMMIC\n",
    "# BRANCH = 'main'\n",
    "# !python -m pip install git+https://github.com/wdika/atommic.git@$BRANCH\n",
    "\n",
    "## Grab the config we'll use in this example\n",
    "!mkdir configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6G2TZkaxcM0e"
   },
   "source": [
    "## Foundations of ATOMMIC\n",
    "---------\n",
    "\n",
    "ATOMMIC models leverage [PyTorch Lightning](https://github.com/PyTorchLightning/pytorch-lightning) Module, and are compatible with the entire PyTorch ecosystem. This means that users have the full flexibility of using the higher level APIs provided by PyTorch Lightning (via Trainer), or write their own training and evaluation loops in PyTorch directly (by simply calling the model and the individual components of the model).\n",
    "\n",
    "For ATOMMIC developers, a \"Model\" is the neural network(s) as well as all the infrastructure supporting those network(s), wrapped into a singular, cohesive unit. As such, all ATOMMIC models are constructed to contain the following out of the box (at the bare minimum, some models support additional functionality too!) -\n",
    "\n",
    " -  Neural Network architecture - all of the modules that are required for the model.\n",
    "\n",
    " -  Dataset + Data Loaders - all of the components that prepare the data for consumption during training or evaluation.\n",
    "\n",
    " -  Preprocessing + Postprocessing - all of the components that process the datasets so they can easily be consumed by the modules.\n",
    "\n",
    " -  Optimizer + Schedulers - basic defaults that work out of the box, and allow further experimentation with ease.\n",
    "\n",
    " - Any other supporting infrastructure - transforms, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-04T21:31:08.311503Z",
     "start_time": "2023-10-04T21:31:08.302495Z"
    },
    "id": "XxAwtqWBQrNk"
   },
   "outputs": [],
   "source": [
    "import atommic\n",
    "atommic.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H01SHfKQh-gV"
   },
   "source": [
    "## ATOMMIC Collections\n",
    "\n",
    "ATOMMIC is sub-divided into a few fundamental collections based on their domains - `mtl`, `qmri`, `rec`, `seg`. When you performed the `import atommic` statement above, none of the above collections were imported. This is because you might not need all of the collections at once, so ATOMMIC allows partial imports of just one or more collection, as and when you require them.\n",
    "\n",
    "-------\n",
    "Let's import the above four collections - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T10:38:17.611621Z",
     "start_time": "2023-10-05T10:38:07.141126Z"
    },
    "id": "J09NNa8fhth7"
   },
   "outputs": [],
   "source": [
    "import atommic.collections.multitask.rs as atommic_mtlrs\n",
    "import atommic.collections.quantitative as atommic_qmri\n",
    "import atommic.collections.reconstruction as atommic_rec\n",
    "import atommic.collections.segmentation as atommic_seg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bSvYoeBrjPza"
   },
   "source": [
    "## ATOMMIC Models in Collections\n",
    "\n",
    "ATOMMIC contains several models for each of its collections. At a brief glance, let's look at all the Models that ATOMMIC offers for the above 4 collections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T10:38:17.662323Z",
     "start_time": "2023-10-05T10:38:17.579559Z"
    },
    "id": "9LbbC_92i41f"
   },
   "outputs": [],
   "source": [
    "mtlrs_models = [model for model in dir(atommic_mtlrs.nn) if not model.startswith(\"__\") and not model.islower() and not \"Block\" in model]\n",
    "mtlrs_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T01:32:02.981725Z",
     "start_time": "2023-10-05T01:32:02.954974Z"
    },
    "id": "t5_ax9Z8j9FC"
   },
   "outputs": [],
   "source": [
    "qmri_models = [model for model in dir(atommic_qmri.nn) if not model.startswith(\"__\") and not model.islower()]\n",
    "qmri_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T01:32:03.272160Z",
     "start_time": "2023-10-05T01:32:03.255055Z"
    },
    "id": "bQdR6RJdkezq"
   },
   "outputs": [],
   "source": [
    "rec_models = [model for model in dir(atommic_rec.nn) if not model.startswith(\"__\") and not model.islower()]\n",
    "rec_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T01:32:03.705541Z",
     "start_time": "2023-10-05T01:32:03.659338Z"
    }
   },
   "outputs": [],
   "source": [
    "seg_models = [model for model in dir(atommic_seg.nn) if not model.startswith(\"__\") and not model.islower()]\n",
    "seg_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iWKxKQnSkj9Z"
   },
   "source": [
    "## The ATOMMIC Model\n",
    "\n",
    "Let's dive deeper into what a ATOMMIC model really is. There are many ways we can create these models - we can use the constructor and pass in a config, we can instantiate the model from a pre-trained checkpoint, or simply pass a pre-trained model name and instantiate a model directly from the cloud !\n",
    "\n",
    "---------\n",
    "For now, let's try to work with a reconstruction UNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n-XOQaW1kh3v"
   },
   "outputs": [],
   "source": [
    "rec_unet = atommic_rec.nn.UNet.from_pretrained('wdika/rec_unet_small_cc359_poisson2d_5x_10x_sense_autoestimationcsm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YP4X7KVPli6g"
   },
   "outputs": [],
   "source": [
    "rec_unet.summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MB91Swu0pIKr"
   },
   "source": [
    "## Model Configuration using OmegaConf\n",
    "--------\n",
    "\n",
    "So we could download, instantiate and analyse the high level structure of the `UNet` model in a few lines! Now let's delve deeper into the configuration file that makes the model work.\n",
    "\n",
    "First, we import [OmegaConf](https://omegaconf.readthedocs.io/en/latest/). OmegaConf is an excellent library that is used throughout ATOMMIC in order to enable us to perform yaml configuration management more easily. Additionally, it plays well with another library, [Hydra](https://hydra.cc/docs/intro/), that is used by ATOMMIC to perform on the fly config edits from the command line, dramatically boosting ease of use of our config files !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RkgrDJvumFER"
   },
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CktakfBluA56"
   },
   "source": [
    "All ATOMMIC models come packaged with their model configuration inside the `cfg` attribute. While technically it is meant to be config declaration of the model as it has been currently constructed, `cfg` is an essential tool to modify the behaviour of the Model after it has been constructed. It can be safely used to make it easier to perform many essential tasks inside Models. \n",
    "\n",
    "To be doubly sure, we generally work on a copy of the config until we are ready to edit it inside the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ISd6z7sXt9Mm"
   },
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N2_SiLHRve8A",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cfg = copy.deepcopy(rec_unet.cfg)\n",
    "print(OmegaConf.to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sIwhdXkwxn6R"
   },
   "source": [
    "## Modifying the contents of the Model config\n",
    "----------\n",
    "\n",
    "Say we want to experiment with a different scheduler to this model during training. \n",
    "\n",
    "OmegaConf makes this a very simple task for us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WlSZ8EA4yGKo",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# OmegaConf won't allow you to add new config items, so we temporarily disable this safeguard.\n",
    "OmegaConf.set_struct(cfg, False)\n",
    "\n",
    "# Let's see the old optim config\n",
    "print(\"Old Config: \")\n",
    "print(OmegaConf.to_yaml(cfg.optim))\n",
    "\n",
    "sched = {'name': 'InverseSquareRootAnnealing', 'warmup_steps': 1000, 'min_lr': 1e-6}\n",
    "sched = OmegaConf.create(sched)  # Convert it into a DictConfig\n",
    "\n",
    "# Assign it to cfg.optim.sched namespace\n",
    "cfg.optim.sched = sched\n",
    "\n",
    "# Let's see the new optim config\n",
    "print(\"New Config: \")\n",
    "print(OmegaConf.to_yaml(cfg.optim))\n",
    "\n",
    "# Here, we restore the safeguards so no more additions can be made to the config\n",
    "OmegaConf.set_struct(cfg, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-nMDN66502kn"
   },
   "source": [
    "## Updating the model from config\n",
    "----------\n",
    "\n",
    "ATOMMIC Models can be updated in a few ways, but we follow similar patterns within each collection so as to maintain consistency.\n",
    "\n",
    "Here, we will show the two most common ways to modify core components of the model - using the `from_config_dict` method, and updating a few special parts of the model.\n",
    "\n",
    "Remember, all ATOMMIC models are PyTorch Lightning modules, which themselves are PyTorch modules, so we have a lot of flexibility here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dsxQHBV86R4a"
   },
   "outputs": [],
   "source": [
    "# Update the model config\n",
    "rec_unet.cfg = cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXRRBnJk5tCv"
   },
   "source": [
    "## Update a few special components of the Model\n",
    "---------\n",
    "\n",
    "While the above approach is good for most major components of the model, ATOMMIC has special utilities for a few components.\n",
    "\n",
    "They are - \n",
    "\n",
    " - `setup_training_data`\n",
    " - `setup_validation_data` and `setup_multi_validation_data`\n",
    " - `setup_test_data` and `setup_multi_test_data`\n",
    " - `setup_optimization`\n",
    "\n",
    "These special utilities are meant to help you easily setup training, validation, testing once you restore a model from a checkpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1hXXdaup-QmG"
   },
   "source": [
    "Let's discuss how to add the scheduler to the model below (which initially had just an optimizer in its config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cveKWvMZ4zBo"
   },
   "outputs": [],
   "source": [
    "# Let's print out the current optimizer\n",
    "print(OmegaConf.to_yaml(rec_unet.cfg.optim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XVguw3k0-f6b"
   },
   "outputs": [],
   "source": [
    "# Now let's update the config\n",
    "rec_unet.setup_optimization(cfg.optim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1JZBCQeW-21X"
   },
   "source": [
    "-------\n",
    "We see a warning - \n",
    "\n",
    "```\n",
    "Neither `max_steps` nor `iters_per_batch` were provided to `optim.sched`, cannot compute effective `max_steps` !\n",
    "    Scheduler will not be instantiated !\n",
    "```\n",
    "\n",
    "We don't have a train dataset setup, nor do we have max_steps in the config. Most ATOMMIC schedulers cannot be instantiated without computing how many train steps actually exist!\n",
    "\n",
    "Here, we can temporarily allow the scheduler construction by explicitly passing a max_steps value to be 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mqC89hfE-tqf"
   },
   "outputs": [],
   "source": [
    "OmegaConf.set_struct(cfg.optim.sched, False)\n",
    "\n",
    "cfg.optim.sched.max_steps = 100\n",
    "\n",
    "OmegaConf.set_struct(cfg.optim.sched, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r22IqOBK_q6l"
   },
   "outputs": [],
   "source": [
    "# Now let's update the config and try again\n",
    "rec_unet.setup_optimization(cfg.optim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U7Eezf_sAVS0"
   },
   "source": [
    "You might wonder why we didnt explicitly set `rec_unet.cfg.optim = cfg.optim`. \n",
    "\n",
    "This is because the `setup_optimization()` method does it for you! You can still update the config manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "THqhXy_lQ7i8"
   },
   "source": [
    "### Optimizer & Scheduler Config\n",
    "\n",
    "Optimizers and schedulers are common components of models, and are essential to train the model from scratch.\n",
    "\n",
    "They are grouped together under a unified `optim` namespace, as schedulers often operate on a given optimizer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6HY51nuoSJs5"
   },
   "source": [
    "### Let's breakdown the general `optim` structure\n",
    "```yaml\n",
    "optim:\n",
    "    name: novograd\n",
    "    lr: 0.01\n",
    "\n",
    "    # optimizer arguments\n",
    "    betas: [0.8, 0.25]\n",
    "    weight_decay: 0.001\n",
    "\n",
    "    # scheduler setup\n",
    "    sched:\n",
    "      name: CosineAnnealing\n",
    "\n",
    "      # Optional arguments\n",
    "      max_steps: -1 # computed at runtime or explicitly set here\n",
    "      monitor: val_loss\n",
    "      reduce_on_plateau: false\n",
    "\n",
    "      # scheduler config override\n",
    "      warmup_steps: 1000\n",
    "      warmup_ratio: null\n",
    "      min_lr: 1e-9\n",
    "```\n",
    "\n",
    "Essential Optimizer components - \n",
    "\n",
    " - `name`: String name of the optimizer. Generally a lower case of the class name.\n",
    " - `lr`: Learning rate is a required argument to all optimizers.\n",
    "\n",
    "Optional Optimizer components - after the above two arguments are provided, any additional arguments added under `optim` will be passed to the constructor of that optimizer as keyword arguments\n",
    "\n",
    " - `betas`: List of beta values to pass to the optimizer\n",
    " - `weight_decay`: Optional weight decay passed to the optimizer.\n",
    "\n",
    "Optional Scheduler components - `sched` is an optional setup of the scheduler for the given optimizer.\n",
    "\n",
    "If `sched` is provided, only one essential argument needs to be provided : \n",
    "\n",
    " - `name`: The name of the scheduler. Generally, it is the full class name.\n",
    "\n",
    "Optional Scheduler components - \n",
    "\n",
    " - `max_steps`: Max steps as an override from the user. If one provides `trainer.max_steps` inside the trainer configuration, that value is used instead. If neither value is set, the scheduler will attempt to compute the `effective max_steps` using the size of the train data loader. If that too fails, then the scheduler will not be created at all.\n",
    "\n",
    " - `monitor`: Used if you are using an adaptive scheduler such as ReduceLROnPlateau. Otherwise ignored. Defaults to `loss` - indicating train loss as monitor.\n",
    "\n",
    " - `reduce_on_plateau`: Required to be set to true if using an adaptive scheduler.\n",
    "\n",
    "Any additional arguments under `sched` will be supplied as keyword arguments to the constructor of the scheduler.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZKURHn0jH_52"
   },
   "source": [
    "## Creating Model from constructor vs restoring a model\n",
    "---------\n",
    "\n",
    "You might notice, we discuss all of the above setup methods in the context of model after it is restored. However, ATOMMIC scripts do not call them inside any of the example train scripts themselves.\n",
    "\n",
    "This is because these methods are automatically called by the constructor when the Model is created for the first time, but these methods are skipped during restoration (either from a PyTorch Lightning checkpoint using `load_from_checkpoint`, or via `restore_from` method inside ATOMMIC Models).\n",
    "\n",
    "This is done as most datasets are stored on a user's local directory, and the path to these datasets is set in the config (either set by default, or set by Hydra overrides). On the other hand, the models are meant to be portable. On another user's system, the data might not be placed at exactly the same location, or even on the same drive as specified in the model's config!\n",
    "\n",
    "Therefore we allow the constructor some brevity and automate such dataset setup, whereas restoration warns that data loaders were not set up and provides the user with ways to set up their own datasets.\n",
    "\n",
    "------\n",
    "\n",
    "Why are optimizers not restored automatically? Well, optimizers themselves don't face an issue, but as we saw before, schedulers depend on the number of train steps in order to calculate their schedule.\n",
    "\n",
    "However, if you don't wish to modify the optimizer and scheduler, and prefer to leave them to their default values, that's perfectly alright. The `setup_optimization()` method is automatically called by PyTorch Lightning for you when you begin training your model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g91FE8mlMcnh"
   },
   "source": [
    "## Saving and restoring models\n",
    "----------\n",
    "\n",
    "ATOMMIC provides a few ways to save and restore models. If you utilize the Experiment Manager that is part of all ATOMMIC train scripts, PyTorch Lightning will automatically save checkpoints for you in the experiment directory.\n",
    "\n",
    "We can also use packaged files using the specialized `save_to` and `restore_from` methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NzMxga7QNYn8"
   },
   "source": [
    "### Saving and Restoring from PTL Checkpoints\n",
    "----------\n",
    "\n",
    "The PyTorch Lightning Trainer object will periodically save checkpoints when the experiment manager is being used during training.\n",
    "\n",
    "PyTorch Lightning checkpoints can then be loaded and evaluated / fine-tuned just as always using the class method `load_from_checkpoint`.\n",
    "\n",
    "For example, restore a UNet model from a checkpoint - \n",
    "\n",
    "```python\n",
    "rec_unet = atommic_rec.nn.UNet.load_from_checkpoint(<path to checkpoint>)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W4YzAG-KOBkZ"
   },
   "source": [
    "### Saving and Restoring from .atommic files\n",
    "----------\n",
    "\n",
    "There are a few models which might require external dependencies to be packaged with them in order to restore them properly.\n",
    "\n",
    "We can use the `save_to` and `restore_from` method to package the entire model + its components into a tarfile. This can then be easily imported by the user and used to restore the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P6_vMSwXNJ74"
   },
   "outputs": [],
   "source": [
    "# Save the model\n",
    "rec_unet.save_to('rec_unet.atommic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HrBhgaqyP4rU"
   },
   "outputs": [],
   "source": [
    "!ls -d -- *.atommic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "Tyht1E0DQGb_"
   },
   "outputs": [],
   "source": [
    "# Restore the model\n",
    "temp_unet = atommic_rec.nn.UNet.restore_from('rec_unet.atommic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dqNpmYYJQS2H"
   },
   "outputs": [],
   "source": [
    "temp_unet.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "A5e42EoiZYjf"
   },
   "outputs": [],
   "source": [
    "# Note that the preprocessor + optimizer config have been preserved after the changes we made !\n",
    "print(OmegaConf.to_yaml(temp_unet.cfg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OI3RxwpcV-UF"
   },
   "source": [
    "Note, that .atommic file is a simple .tar.gz with checkpoint, configuration and, potentially, other artifacts being used by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jFBAGcaDWLiu"
   },
   "outputs": [],
   "source": [
    "!cp rec_unet.atommic rec_unet.tar.gz\n",
    "!tar -xvf rec_unet.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mkau4Q9jZo1l"
   },
   "source": [
    "### Extracting PyTorch checkpoints from ATOMMIC tarfiles (Model level)\n",
    "-----------\n",
    "\n",
    "While the .atommic tarfile is an excellent way to have a portable model, sometimes it is necessary for researchers to have access to the basic PyTorch save format. ATOMMIC aims to be entirely compatible with PyTorch, and therefore offers a simple method to extract just the PyTorch checkpoint from the .atommic tarfile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qccPANeycCoq"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A4zswOKHar9q"
   },
   "outputs": [],
   "source": [
    "state_dict = temp_unet.extract_state_dict_from('rec_unet.atommic', save_dir='./pt_ckpt/')\n",
    "!ls ./pt_ckpt/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ACB-0dfnbFG3"
   },
   "source": [
    "As we can see below, there is now a single basic PyTorch checkpoint available inside the `pt_ckpt` directory, which we can use to load the weights of the entire model as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ZAF_A0uc5bB"
   },
   "outputs": [],
   "source": [
    "temp_unet.load_state_dict(torch.load('./pt_ckpt/model_weights.ckpt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hkq6EM99cS6y"
   },
   "source": [
    "### Extracting PyTorch checkpoints from ATOMMIC tarfiles (Module level)\n",
    "----------\n",
    "\n",
    "While the above method is exceptional when extracting the checkpoint of the entire model, sometimes there may be a necessity to load and save the individual modules that comprise the Model.\n",
    "\n",
    "The same extraction method offers a flag to extract the individual model level checkpoints into their individual files, so that users have access to per-module level checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LW6wve2zbT9D"
   },
   "outputs": [],
   "source": [
    "state_dict = temp_unet.extract_state_dict_from('rec_unet.atommic', save_dir='./pt_module_ckpt/', split_by_module=True)\n",
    "!ls ./pt_module_ckpt/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "88vOGV7VYcuu"
   },
   "source": [
    "# ATOMMIC with Hydra\n",
    "\n",
    "[Hydra](https://hydra.cc/docs/intro/) is used throughout ATOMMIC as a way to enable rapid prototyping using predefined config files. Hydra and OmegaConf offer great compatibility with each other when using ATOMMIC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optionally you might want to remove any generated files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_directory = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all files in the folder\n",
    "all_files = os.listdir(current_directory)\n",
    "\n",
    "# List all files and directories in the folder\n",
    "for root, dirs, files in os.walk(current_directory, topdown=False):\n",
    "    for filename in files:\n",
    "        file_path = os.path.join(root, filename)\n",
    "        if not filename.endswith(\".ipynb\"):\n",
    "            os.remove(file_path)\n",
    "    for dir_name in dirs:\n",
    "        dir_path = os.path.join(root, dir_name)\n",
    "        if not any(file.endswith(\".ipynb\") for file in os.listdir(dir_path)):\n",
    "            shutil.rmtree(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove .ipynb checkpoints\n",
    "for root, dirs, files in os.walk(current_directory, topdown=False):\n",
    "    for dir_name in dirs:\n",
    "        if dir_name == \".ipynb_checkpoints\":\n",
    "            checkpoint_dir = os.path.join(root, dir_name)\n",
    "            shutil.rmtree(checkpoint_dir)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "00_NeMo_Primer.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
