{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2024-01-22T16:38:26.897922Z",
     "end_time": "2024-01-22T16:38:26.956536Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from typing import Union, Sequence\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import RegularGridInterpolator\n",
    "import h5py\n",
    "from skimage.color import label2rgb\n",
    "import json\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# Function for plotting MRI scans\n",
    "\n",
    "\n",
    "def get_scaled_image(\n",
    "        x: Union[torch.Tensor, np.ndarray], percentile=0.99, clip=False\n",
    "):\n",
    "    \"\"\"Scales image by intensity percentile (and optionally clips to [0, 1]).\n",
    "\n",
    "    Args:\n",
    "      x (torch.Tensor | np.ndarray): The image to process.\n",
    "      percentile (float): The percentile of magnitude to scale by.\n",
    "      clip (bool): If True, clip values between [0, 1]\n",
    "\n",
    "    Returns:\n",
    "      torch.Tensor | np.ndarray: The scaled image.\n",
    "    \"\"\"\n",
    "    is_numpy = isinstance(x, np.ndarray)\n",
    "    if is_numpy:\n",
    "        x = torch.as_tensor(x)\n",
    "\n",
    "    scale_factor = torch.quantile(x, percentile)\n",
    "    x = x / scale_factor\n",
    "    if clip:\n",
    "        x = torch.clip(x, 0, 1)\n",
    "\n",
    "    if is_numpy:\n",
    "        x = x.numpy()\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def plot_images(\n",
    "        images, processor=None, disable_ticks=True, titles: Sequence[str] = None,\n",
    "        ylabel: str = None, xlabels: Sequence[str] = None, cmap: str = \"gray\",\n",
    "        show_cbar: bool = False, overlay=None, opacity: float = 0.3,\n",
    "        hsize=5, wsize=5, axs=None, fontsize=20\n",
    "):\n",
    "    \"\"\"Plot multiple images in a single row.\n",
    "\n",
    "    Add an overlay with the `overlay=` argument.\n",
    "    Add a colorbar with `show_cbar=True`.\n",
    "    \"\"\"\n",
    "\n",
    "    def get_default_values(x, default=\"\"):\n",
    "        if x is None:\n",
    "            return [default] * len(images)\n",
    "        return x\n",
    "\n",
    "    titles = get_default_values(titles)\n",
    "    ylabels = get_default_values(images)\n",
    "    xlabels = get_default_values(xlabels)\n",
    "\n",
    "    N = len(images)\n",
    "    if axs is None:\n",
    "        fig, axs = plt.subplots(1, N, figsize=(wsize * N, hsize))\n",
    "    else:\n",
    "        assert len(axs) >= N\n",
    "        fig = axs.flatten()[0].get_figure()\n",
    "    k = 0\n",
    "    for ax, img, title, xlabel in zip(axs, images, titles, xlabels):\n",
    "        if processor is not None:\n",
    "            img = processor(img)\n",
    "        if type(cmap) == list:\n",
    "            im = ax.imshow(img, cmap=cmap[k])\n",
    "            if type(show_cbar) == list:\n",
    "                if show_cbar[k]:\n",
    "                    fig.subplots_adjust(right=0.8)\n",
    "                    cbar_ax = fig.add_axes([0.85, 0.2, 0.01, 0.60])\n",
    "                    fig.colorbar(im, cax=cbar_ax)\n",
    "        else:\n",
    "            im = ax.imshow(img, cmap=cmap)\n",
    "        k = k + 1\n",
    "        ax.set_title(title, fontsize=fontsize)\n",
    "        ax.set_xlabel(xlabel)\n",
    "\n",
    "    if type(overlay) == list:\n",
    "        for i, ax in enumerate(axs.flatten()):\n",
    "            if overlay[i] is not None:\n",
    "                im = ax.imshow(overlay[i], alpha=opacity)\n",
    "\n",
    "    if disable_ticks:\n",
    "        for ax in axs.flatten():\n",
    "            ax.get_xaxis().set_ticks([])\n",
    "            ax.get_yaxis().set_ticks([])\n",
    "\n",
    "    return axs\n",
    "\n",
    "\n",
    "# Function for transforming segmentation classes into one hot\n",
    "def categorical_to_one_hot(x, channel_dim: int = 1, background=0, num_categories=None, dtype=None):\n",
    "    \"\"\"Converts categorical predictions to one-hot encoded predictions.\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor | np.ndarray): Categorical array or tensor.\n",
    "        channel_dim (int, optional): Channel dimension for output tensor.\n",
    "        background (int | NoneType, optional): The numerical label of the\n",
    "            background category. If ``None``, assumes that the background is\n",
    "            a class that should be one-hot encoded.\n",
    "        num_categories (int, optional): Number of categories (excluding background).\n",
    "            Defaults to the ``max(x) + 1``.\n",
    "        dtype (type, optional): Data type of the output.\n",
    "            Defaults to boolean (``torch.bool`` or ``np.bool``).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor | np.ndarray: One-hot encoded predictions.\n",
    "    \"\"\"\n",
    "    is_ndarray = isinstance(x, np.ndarray)\n",
    "    if is_ndarray:\n",
    "        x = torch.from_numpy(x)\n",
    "\n",
    "    if num_categories is None:\n",
    "        num_categories = torch.max(x).type(torch.long).cpu().item()\n",
    "    num_categories += 1\n",
    "\n",
    "    shape = x.shape\n",
    "    out_shape = (num_categories,) + shape\n",
    "\n",
    "    if dtype is None:\n",
    "        dtype = torch.bool\n",
    "    default_value = True if dtype == torch.bool else 1\n",
    "    if x.dtype != torch.long:\n",
    "        x = x.type(torch.long)\n",
    "\n",
    "    out = torch.zeros(out_shape, dtype=dtype, device=x.device)\n",
    "    out.scatter_(0, x.reshape((1,) + x.shape), default_value)\n",
    "    if background is not None:\n",
    "        out = torch.cat([out[0:background], out[background + 1:]], dim=0)\n",
    "    if channel_dim != 0:\n",
    "        if channel_dim < 0:\n",
    "            channel_dim = out.ndim + channel_dim\n",
    "        order = (channel_dim,) + tuple(d for d in range(out.ndim) if d != channel_dim)\n",
    "        out = out.permute(tuple(np.argsort(order)))\n",
    "        out = out.contiguous()\n",
    "\n",
    "    if is_ndarray:\n",
    "        out = out.numpy()\n",
    "    return out\n",
    "\n",
    "\n",
    "# Function for combining one hot encoded classes\n",
    "def collect_mask(\n",
    "        mask: np.ndarray,\n",
    "        index: Sequence[Union[int, Sequence[int], int]],\n",
    "        out_channel_first: bool = True,\n",
    "):\n",
    "    \"\"\"Collect masks by index.\n",
    "\n",
    "    Collated indices will be summed. For example, `index=(1,(3,4))` will return\n",
    "    `np.stack(mask[...,1], mask[...,3]+mask[...,4])`.\n",
    "\n",
    "    TODO: Add support for adding background.\n",
    "\n",
    "    Args:\n",
    "        mask (ndarray): A (...)xC array.\n",
    "        index (Sequence[int]): The index/indices to select in mask.\n",
    "            If sub-indices are collated, they will be summed.\n",
    "        out_channel_first (bool, optional): Reorders dimensions of output mask to Cx(...)\n",
    "    \"\"\"\n",
    "    if isinstance(index, int):\n",
    "        index = (index,)\n",
    "\n",
    "    if not any(isinstance(idx, Sequence) for idx in index):\n",
    "        mask = mask[..., index]\n",
    "    else:\n",
    "        o_seg = []\n",
    "        for idx in index:\n",
    "            c_seg = mask[..., idx]\n",
    "            if isinstance(idx, Sequence):\n",
    "                c_seg = np.sum(c_seg, axis=-1)\n",
    "            o_seg.append(c_seg)\n",
    "        mask = np.stack(o_seg, axis=-1)\n",
    "\n",
    "    if out_channel_first:\n",
    "        last_idx = len(mask.shape) - 1\n",
    "        mask = np.transpose(mask, (last_idx,) + tuple(range(0, last_idx)))\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "# Function for transforming segmentation classes into categorical\n",
    "def one_hot_to_categorical(x, channel_dim: int = 1, background=False):\n",
    "    \"\"\"Converts one-hot encoded predictions to categorical predictions.\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor | np.ndarray): One-hot encoded predictions.\n",
    "        channel_dim (int, optional): Channel dimension.\n",
    "            Defaults to ``1`` (i.e. ``(B,C,...)``).\n",
    "        background (bool, optional): If ``True``, assumes index 0 in the\n",
    "            channel dimension is the background.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor | np.ndarray: Categorical array or tensor. If ``background=False``,\n",
    "        the output will be 1-indexed such that ``0`` corresponds to the background.\n",
    "    \"\"\"\n",
    "    is_ndarray = isinstance(x, np.ndarray)\n",
    "    if is_ndarray:\n",
    "        x = torch.as_tensor(x)\n",
    "\n",
    "    if background is not None and background is not False:\n",
    "        out = torch.argmax(x, channel_dim)\n",
    "    else:\n",
    "        out = torch.argmax(x.type(torch.long), dim=channel_dim) + 1\n",
    "        out = torch.where(x.sum(channel_dim) == 0, torch.tensor([0], device=x.device), out)\n",
    "\n",
    "    if is_ndarray:\n",
    "        out = out.numpy()\n",
    "    return out"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-22T16:38:27.738458Z",
     "end_time": "2024-01-22T16:38:27.766685Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MTR_013\n",
      "<KeysViewHDF5 ['kspace', 'maps', 'masks', 'target']>\n",
      "(512, 512, 160, 2, 8) (512, 512, 160, 4) (512, 512, 160, 8, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load the corresponding data\n",
    "dataset_dir = '/data/projects/recon/data/public/multitask/skm-tea/v1-release/'\n",
    "meta_dir = Path(dataset_dir) / 'all_metadata.csv'\n",
    "meta_data = pd.read_csv(meta_dir)\n",
    "\n",
    "recon_file = Path(dataset_dir) / \"files_recon_calib-24/\"\n",
    "\n",
    "seg_file = Path(dataset_dir)  / str(\"segmentation_masks/raw-data-track/\")\n",
    "\n",
    "patient_id = 'MTR_013' #meta_data['MTR_ID'][60]\n",
    "print(patient_id)\n",
    "meta_data = meta_data[meta_data['MTR_ID']==patient_id]\n",
    "\n",
    "with h5py.File(str(recon_file / patient_id)+'.h5', \"r\") as f:\n",
    "    print(f.keys())\n",
    "    kspace_org = f[\"kspace\"][:, :, :, :, :]  # Shape: (x, ky, kz, #echos, #coils)\n",
    "    maps_org = f[\"maps\"][:, :, :, :, :]      # Shape: (x, ky, kz, #coils, #maps) - maps are the same for both echos\n",
    "    image_org = f['target'][()]\n",
    "\n",
    "segmentation = nib.load(str(seg_file / patient_id)+'.nii.gz').get_fdata()\n",
    "segmentation_one = categorical_to_one_hot(segmentation,channel_dim=-1)\n",
    "segmentation_one_org = collect_mask(segmentation_one, (0, 1, (2, 3), (4, 5)), out_channel_first=False)\n",
    "crop_scale = [1,1,1] #Option to crop\n",
    "print(kspace_org.shape, segmentation_one_org.shape, maps_org.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-22T16:38:31.294474Z",
     "end_time": "2024-01-22T16:38:52.333689Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['info', 'categories', 'tissues', 'images', 'annotations'])\n",
      "[{'supercategory': 'Meniscal Tear', 'supercategory_id': 1, 'id': 1, 'name': 'Meniscal Tear (Myxoid)'}, {'supercategory': 'Meniscal Tear', 'supercategory_id': 1, 'id': 2, 'name': 'Meniscal Tear (Horizontal)'}, {'supercategory': 'Meniscal Tear', 'supercategory_id': 1, 'id': 3, 'name': 'Meniscal Tear (Radial)'}, {'supercategory': 'Meniscal Tear', 'supercategory_id': 1, 'id': 4, 'name': 'Meniscal Tear (Vertical/Longitudinal)'}, {'supercategory': 'Meniscal Tear', 'supercategory_id': 1, 'id': 5, 'name': 'Meniscal Tear (Oblique)'}, {'supercategory': 'Meniscal Tear', 'supercategory_id': 1, 'id': 6, 'name': 'Meniscal Tear (Complex)'}, {'supercategory': 'Meniscal Tear', 'supercategory_id': 1, 'id': 7, 'name': 'Meniscal Tear (Flap)'}, {'supercategory': 'Meniscal Tear', 'supercategory_id': 1, 'id': 8, 'name': 'Meniscal Tear (Extrusion)'}, {'supercategory': 'Ligament Tear', 'supercategory_id': 2, 'id': 9, 'name': 'Ligament Tear (Low-Grade Sprain)'}, {'supercategory': 'Ligament Tear', 'supercategory_id': 2, 'id': 10, 'name': 'Ligament Tear (Moderate Grade Sprain or Mucoid Degeneration)'}, {'supercategory': 'Ligament Tear', 'supercategory_id': 2, 'id': 11, 'name': 'Ligament Tear (Full Thickness/Complete Tear)'}, {'supercategory': 'Cartilage Lesion', 'supercategory_id': 3, 'id': 12, 'name': 'Cartilage Lesion (1)'}, {'supercategory': 'Cartilage Lesion', 'supercategory_id': 3, 'id': 13, 'name': 'Cartilage Lesion (2A)'}, {'supercategory': 'Cartilage Lesion', 'supercategory_id': 3, 'id': 14, 'name': 'Cartilage Lesion (2B)'}, {'supercategory': 'Cartilage Lesion', 'supercategory_id': 3, 'id': 15, 'name': 'Cartilage Lesion (3)'}, {'supercategory': 'Effusion', 'supercategory_id': 4, 'id': 16, 'name': 'Effusion'}]\n",
      "5    48\n",
      "Name: PatientID, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "with open('/data/projects/recon/data/public/multitask/skm-tea/v1-release/annotations/v1.0.0/train.json', 'r') as f:\n",
    "    object = json.load(f)\n",
    "\n",
    "print(object.keys())\n",
    "print(object['categories'])\n",
    "print(meta_data['PatientID'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-22T17:11:30.634639Z",
     "end_time": "2024-01-22T17:11:30.645986Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/tmpaquaij/.cache/torch/hub/ultralytics_yolov5_master\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhub\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43multralytics/yolov5\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43myolov5s\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpretrained\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/scratch/tmpaquaij/Conda/atommic/lib/python3.10/site-packages/torch/hub.py:558\u001B[0m, in \u001B[0;36mload\u001B[0;34m(repo_or_dir, model, source, trust_repo, force_reload, verbose, skip_validation, *args, **kwargs)\u001B[0m\n\u001B[1;32m    554\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m source \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgithub\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m    555\u001B[0m     repo_or_dir \u001B[38;5;241m=\u001B[39m _get_cache_or_reload(repo_or_dir, force_reload, trust_repo, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mload\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    556\u001B[0m                                        verbose\u001B[38;5;241m=\u001B[39mverbose, skip_validation\u001B[38;5;241m=\u001B[39mskip_validation)\n\u001B[0;32m--> 558\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43m_load_local\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrepo_or_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    559\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m model\n",
      "File \u001B[0;32m/scratch/tmpaquaij/Conda/atommic/lib/python3.10/site-packages/torch/hub.py:587\u001B[0m, in \u001B[0;36m_load_local\u001B[0;34m(hubconf_dir, model, *args, **kwargs)\u001B[0m\n\u001B[1;32m    584\u001B[0m     hub_module \u001B[38;5;241m=\u001B[39m _import_module(MODULE_HUBCONF, hubconf_path)\n\u001B[1;32m    586\u001B[0m     entry \u001B[38;5;241m=\u001B[39m _load_entry_from_hubconf(hub_module, model)\n\u001B[0;32m--> 587\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[43mentry\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    589\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m model\n",
      "File \u001B[0;32m~/.cache/torch/hub/ultralytics_yolov5_master/hubconf.py:98\u001B[0m, in \u001B[0;36myolov5s\u001B[0;34m(pretrained, channels, classes, autoshape, _verbose, device)\u001B[0m\n\u001B[1;32m     96\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21myolov5s\u001B[39m(pretrained\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, channels\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m, classes\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m80\u001B[39m, autoshape\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, _verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, device\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m     97\u001B[0m     \u001B[38;5;66;03m# YOLOv5-small model https://github.com/ultralytics/yolov5\u001B[39;00m\n\u001B[0;32m---> 98\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_create\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43myolov5s\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpretrained\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchannels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclasses\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mautoshape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_verbose\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.cache/torch/hub/ultralytics_yolov5_master/hubconf.py:34\u001B[0m, in \u001B[0;36m_create\u001B[0;34m(name, pretrained, channels, classes, autoshape, verbose, device)\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;124;03mCreates or loads a YOLOv5 model.\u001B[39;00m\n\u001B[1;32m     19\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;124;03m    YOLOv5 model\u001B[39;00m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpathlib\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Path\n\u001B[0;32m---> 34\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmodels\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcommon\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AutoShape, DetectMultiBackend\n\u001B[1;32m     35\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmodels\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m attempt_load\n\u001B[1;32m     36\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmodels\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01myolo\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ClassificationModel, DetectionModel, SegmentationModel\n",
      "File \u001B[0;32m~/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:16\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpathlib\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Path\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01murllib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mparse\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m urlparse\n\u001B[0;32m---> 16\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mcv2\u001B[39;00m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
