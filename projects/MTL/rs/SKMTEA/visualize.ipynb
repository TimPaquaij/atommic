{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oeT5uJROMl6L"
   },
   "source": [
    "# üçµ SKM-TEA Dataset Tutorial\n",
    "[Paper](https://arxiv.org/abs/2203.06823) | [GitHub](https://github.com/StanfordMIMI/skm-tea)\n",
    "\n",
    "Welcome to the SKM-TEA dataset demo!\n",
    "\n",
    "**Dataset**: The *Stanford Knee MRI with Multi-Task Evaluation (SKM-TEA) dataset* is a collection of quantitative knee MRI scans that enables end-to-end benchmarking of MRI reconstruction and analysis methods. This 1.6TB dataset consists of raw-data measurements of ~25,000 slices (155 patients) of anonymized patient MRI scans, the corresponding scanner-generated DICOM images, manual segmentations of four tissues, and bounding box annotations for sixteen clinically relevant pathologies.\n",
    "\n",
    "**Brief**: In this demo, we will walk through the data and how to use [the codebase](https://github.com/StanfordMIMI/skm-tea) to run pre-trained models and perform evaluation with your own methods.\n",
    "\n",
    "- Inspect different data types in SKM-TEA *DICOM* and *Raw Data* Tracks\n",
    "- Use pretrained models from the [model zoo](https://github.com/StanfordMIMI/skm-tea/blob/main/MODEL_ZOO.md)\n",
    "- Perform clinically-relevant quantitative MRI (qMRI) evaluation\n",
    "\n",
    "Interested in learning how to train models with SKM-TEA, check out [this tutorial](https://colab.research.google.com/drive/1LUC0MqFYK39xG5AV9kQi5hIBsi9eCpS0?usp=sharing)\n",
    "\n",
    "**Time**: 25-30 minutes\n",
    "\n",
    "**Colab Runtime**: We recommend running this Colab with a GPU runtime. To change the runtime,\n",
    "1. Click on `Runtime` on the top navigation bar\n",
    "2. Select `Change runtime type`\n",
    "3. Select `GPU` from the dropdown\n",
    "\n",
    "**NOTE**: This tutorial is under development. Please contact the arjundd \\<at stanford email domain\\> with any bugs or recommendations.\n",
    "\n",
    "**Acknowledgements**: SKM-TEA is built on the [Meddlr](https://github.com/ad12/meddlr) image reconstruction and analysis framework.\n",
    "\n",
    "**Coming Soon:**\n",
    "- Tutorial with detection (bounding box) labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5g8MtjY5Vs7c"
   },
   "source": [
    "## üì° Downloading the Data\n",
    "Let's download a [mini version](https://huggingface.co/datasets/arjundd/skm-tea-mini) of the SKM-TEA dataset from Huggingface. This mini dataset was created for building demos/tutorials with the SKM-TEA dataset. **Do not use this dataset for reporting/publication purposes**\n",
    "\n",
    "*NOTE*: This download process can take ~5-8 minutes.\n",
    "\n",
    "> If you would like to set up up the full SKM-TEA dataset on your machine, follow [these instructions](https://github.com/StanfordMIMI/skm-tea/blob/main/DATASET.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mLWsy6TS6KGX",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "422b0934-856c-4bf2-9c92-692be5371d02",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "dataset_dir = \"skm-tea/v1-release\"\n",
    "url = \"https://huggingface.co/datasets/arjundd/skm-tea-mini/resolve/main/v1-release\"\n",
    "force_download = False\n",
    "\n",
    "if force_download:\n",
    "  !rm -rf $dataset_dir\n",
    "\n",
    "if not os.path.isdir(dataset_dir):\n",
    "  os.makedirs(dataset_dir)\n",
    "  for fname in [\"all_metadata.csv\", \"annotations/v1.0.0/train.json\", \"annotations/v1.0.0/val.json\", \"annotations/v1.0.0/test.json\"]:\n",
    "    out = f\"{dataset_dir}/{fname}\"\n",
    "    os.makedirs(os.path.dirname(out), exist_ok=True)\n",
    "    !wget -q $url/$fname -O $out\n",
    "\n",
    "\n",
    "  for fname in tqdm([\"dicoms\", \"files_recon_calib-24\", \"image_files\", \"segmentation_masks\"], disable=False):\n",
    "    !wget -c $url/\"tarball\"/$fname\".tar.gz\" -O - | tar -xz -C $dataset_dir/\n",
    "\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TfqPa76WMl6P"
   },
   "source": [
    "## üöß Setup\n",
    "All SKM-TEA code for training, evaluation, models, and more ships as a Python package. In this tutorial, we will learn how to use different parts of this package.\n",
    "\n",
    "> To use the latest version from the `main` branch, use `pip install git+https://github.com/StanfordMIMI/skm-tea.git`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s76_F_6FYVoK",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "c9e80dea-5841-4e4a-b179-c81fbfefc9df"
   },
   "outputs": [],
   "source": [
    "# Download SKM-TEA from main branch on GitHub\n",
    "!pip install --upgrade pytorch-lightning==1.7.7 skm-tea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "968_Ac3sMl6O"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"MEDDLR_DATASETS_DIR\"] = \"./\"\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.color import label2rgb\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "\n",
    "import dosma as dm\n",
    "\n",
    "import meddlr.ops as oF\n",
    "from meddlr.data import DatasetCatalog, MetadataCatalog\n",
    "from meddlr.utils.logger import setup_logger\n",
    "from meddlr.utils import env\n",
    "\n",
    "import skm_tea as st"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Set the default device if cuda is enabled\n",
    "if torch.cuda.is_available():\n",
    "  DEVICE = torch.device(\"cuda\")\n",
    "else:\n",
    "  DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Device: \", DEVICE)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B6st9z4rm1tO",
    "outputId": "66c126bd-fadd-438a-d2a4-f5581cda00dc"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hb5pCSZzMl6Q"
   },
   "outputs": [],
   "source": [
    "# Run this setup phase only once.\n",
    "# Otherwise, you may get multiple print statements\n",
    "setup_logger()\n",
    "logger = setup_logger(\"skm_tea\")\n",
    "path_mgr = env.get_path_manager()"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Some general utilities\n",
    "\n",
    "from typing import Union, Sequence\n",
    "\n",
    "def get_scaled_image(\n",
    "    x: Union[torch.Tensor, np.ndarray], percentile=0.99, clip=False\n",
    "):\n",
    "  \"\"\"Scales image by intensity percentile (and optionally clips to [0, 1]).\n",
    "\n",
    "  Args:\n",
    "    x (torch.Tensor | np.ndarray): The image to process.\n",
    "    percentile (float): The percentile of magnitude to scale by.\n",
    "    clip (bool): If True, clip values between [0, 1]\n",
    "\n",
    "  Returns:\n",
    "    torch.Tensor | np.ndarray: The scaled image.\n",
    "  \"\"\"\n",
    "  is_numpy = isinstance(x, np.ndarray)\n",
    "  if is_numpy:\n",
    "    x = torch.as_tensor(x)\n",
    "\n",
    "  scale_factor = torch.quantile(x, percentile)\n",
    "  x = x / scale_factor\n",
    "  if clip:\n",
    "    x = torch.clip(x, 0, 1)\n",
    "\n",
    "  if is_numpy:\n",
    "    x = x.numpy()\n",
    "\n",
    "  return x\n",
    "\n",
    "\n",
    "def plot_images(\n",
    "    images, processor=None, disable_ticks=True, titles: Sequence[str]=None,\n",
    "    ylabel: str=None, xlabels: Sequence[str]=None, cmap: str=\"gray\",\n",
    "    show_cbar: bool = False, overlay = None, opacity: float = 0.3,\n",
    "    hsize=5, wsize=5, axs=None\n",
    "):\n",
    "  \"\"\"Plot multiple images in a single row.\n",
    "\n",
    "  Add an overlay with the `overlay=` argument.\n",
    "  Add a colorbar with `show_cbar=True`.\n",
    "  \"\"\"\n",
    "  def get_default_values(x, default=\"\"):\n",
    "    if x is None:\n",
    "      return [default] * len(images)\n",
    "    return x\n",
    "\n",
    "  titles = get_default_values(titles)\n",
    "  ylabels = get_default_values(images)\n",
    "  xlabels = get_default_values(xlabels)\n",
    "\n",
    "  N = len(images)\n",
    "  if axs is None:\n",
    "    fig, axs = plt.subplots(1, N, figsize=(wsize * N, hsize))\n",
    "  else:\n",
    "    assert len(axs) >= N\n",
    "    fig = axs.flatten()[0].get_figure()\n",
    "\n",
    "  for ax, img, title, xlabel in zip(axs, images, titles, xlabels):\n",
    "    if processor is not None:\n",
    "      img = processor(img)\n",
    "    im = ax.imshow(img, cmap=cmap)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(xlabel)\n",
    "\n",
    "  if overlay is not None:\n",
    "    for ax in axs.flatten():\n",
    "      im = ax.imshow(overlay, alpha=opacity)\n",
    "\n",
    "  if show_cbar:\n",
    "    fig.subplots_adjust(right=0.8)\n",
    "    cbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])\n",
    "    fig.colorbar(im, cax=cbar_ax)\n",
    "\n",
    "  if disable_ticks:\n",
    "    for ax in axs.flatten():\n",
    "      ax.get_xaxis().set_ticks([])\n",
    "      ax.get_yaxis().set_ticks([])\n",
    "\n",
    "  return axs\n"
   ],
   "metadata": {
    "id": "rjVszyf4aDBj"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2sDPAp83Ml6Q"
   },
   "source": [
    "## üíæ Understanding the Data\n",
    "The SKM-TEA dataset consists of two *tracks* that are based on the source of the input image: the *Raw Data* track, where inputs start from the complex-valued k-space, and the *DICOM* track, where inputs start from magnitude DICOM images.\n",
    "\n",
    "Note, the Raw Data track supports all reconstruction (upstream) and image analysis (downstream) tasks available in SKM-TEA with the caveat that all downstream tasks are performed on the image reconstructed from the raw data.\n",
    "\n",
    "In contrast, the DICOM track only supports image analysis tasks -- it does not support the reconstruction tasks. Read [this paper](https://arxiv.org/abs/2109.08237) for more information on why DICOM images may not be good targets for measuring reconstruction performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `skm_tea` package simplifies getting relevant data paths and metadata using the `DatasetCatalog` manager. We can load any of the dataset splits:\n",
    "- `'skmtea_v1_train'`\n",
    "- `'skmtea_v1_val'`\n",
    "- `'skmtea_v1_test'`"
   ],
   "metadata": {
    "id": "0GVy26yPEaNB"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JCJHRjU9Ml6R"
   },
   "outputs": [],
   "source": [
    "# Load list of dictionaries for the SKM-TEA v1 training dataset.\n",
    "dataset_dicts = DatasetCatalog.get(\"skmtea_v1_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QIjEQX_gMl6R",
    "outputId": "0d02e6a3-d938-41d6-f7ad-8618ec29b76e"
   },
   "outputs": [],
   "source": [
    "scan = dataset_dicts[0]\n",
    "pprint(scan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFeAVfimMl6S"
   },
   "source": [
    "### Raw Data Track\n",
    "The raw data track consists of (1) multi-coil kspace, (2) complex-valued ground truth reconstructions, (3) sensitivity maps, (4) gradient-warp corrected segmentations, and (5) localized bounding boxes for knee pathologies.\n",
    "\n",
    "While qDESS is a 3D sequence, the SI (axial) readout dimension is fully-sampled, and can be reconstructed without information loss using the 1D inverse fast Fourier transform (ifft). Thus, reconstructions are performed on 2D axial ($k_y \\times k_z$) slices.\n",
    "\n",
    "Also, note that the reference segmentations for the raw data track are different than those for the DICOM track to correct for DICOM-specfic post-processing. See [our paper](https://arxiv.org/abs/2203.06823) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iYPX8955Ml6S",
    "outputId": "aee45171-f99a-4965-a850-1a8707563631"
   },
   "outputs": [],
   "source": [
    "sl = 200  # the slice to be plotted\n",
    "\n",
    "# Reconstruction data\n",
    "recon_file = scan[\"recon_file\"]\n",
    "with h5py.File(recon_file, \"r\") as f:\n",
    "    kspace = f[\"kspace\"][sl, :, :, :, :]  # Shape: (x, ky, kz, #echos, #coils)\n",
    "    image = f[\"target\"][sl, :, :, :, :]   # Shape: (x, ky, kz, #echos, #maps) - #maps = 1 for SKM-TEA\n",
    "    maps = f[\"maps\"][sl, :, :, :, :]      # Shape: (x, ky, kz, #coils, #maps) - maps are the same for both echos\n",
    "\n",
    "# Segmentation data\n",
    "seg_file = scan[\"gw_corr_mask_file\"]\n",
    "segmentation = dm.read(seg_file).A[sl, ...]  # Shape: (x, y, z)\n",
    "print(segmentation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 597
    },
    "id": "h_G_os2pMl6T",
    "outputId": "ffd7c9ba-732d-4af7-b6c8-e2b1a8d1a534"
   },
   "outputs": [],
   "source": [
    "# Display kspace per coil\n",
    "n_coils = kspace.shape[-1]\n",
    "nrows = 2\n",
    "hsize = 5\n",
    "wsize = hsize / kspace.shape[0] * kspace.shape[1]\n",
    "_, axs = plt.subplots(nrows, n_coils, figsize=(n_coils * wsize, nrows * hsize))\n",
    "\n",
    "for echo in range(2):\n",
    "  kspace_coils = [np.abs(kspace[..., echo, idx]) for idx in range(n_coils)]\n",
    "  # Scale the kspace to avoid over-saturating the image with center kspace\n",
    "  kspace_coils = [get_scaled_image(x, 0.95, clip=True) for x in kspace_coils]\n",
    "\n",
    "  titles = [f\"Coil {idx+1}\" for idx in range(n_coils)] if echo==0 else None\n",
    "  plot_images(kspace_coils, titles=titles, axs=axs[echo])\n",
    "  axs[echo][0].set_ylabel(\"Echo {}\".format(echo + 1), fontsize=20)\n",
    "\n",
    "plt.subplots_adjust(wspace=0.1, hspace=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 327
    },
    "id": "EGowchm2Ml6T",
    "outputId": "fc3497cb-2e68-4da8-e38e-dd3f46c6b51e"
   },
   "outputs": [],
   "source": [
    "# Plot reconstructed image\n",
    "mag_img = np.abs(image)\n",
    "seg_colorized = label2rgb(segmentation, bg_label=0)\n",
    "\n",
    "\n",
    "_ = plot_images(\n",
    "  [mag_img[..., 0, 0], mag_img[..., 0, 0]],  # echo1, echo2\n",
    "  processor=lambda x: get_scaled_image(x, 0.95, clip=True),\n",
    "  titles=[\"Echo 1\", \"Echo 2\"],\n",
    "  overlay=seg_colorized,\n",
    "  opacity=0.4,\n",
    "  hsize=5, wsize=2.3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lpmPd77iMl6U"
   },
   "source": [
    "### DICOM Track\n",
    "The DICOM Track consists of (1) scanner-generated DICOM images, (2) tissue segmentations, and (3) pathology bounding boxes.\n",
    "\n",
    "**IMPORTANT**: As mentioned above, this data should only be used for image analysis (segmentation, detection, classification) tasks. It should not be used for reconstruction tasks.\n",
    "\n",
    "\n",
    "Let's visualize a sagittal slice from both echos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PwtalV6eMl6U"
   },
   "outputs": [],
   "source": [
    "sl = 60  # the slice to be plotted\n",
    "\n",
    "# DICOM data + segmentation\n",
    "image_file = scan[\"image_file\"]\n",
    "with h5py.File(image_file, \"r\") as f:\n",
    "    echo1 = f[\"echo1\"][:, :, sl]  # Shape: (x, y, z)\n",
    "    echo2 = f[\"echo2\"][:, :, sl]  # Shape: (x, y, z)\n",
    "    segmentation = f[\"seg\"][:, :, sl, :]  # Shape: (x, y, z, #classes)\n",
    "\n",
    "segmentation = oF.one_hot_to_categorical(segmentation, channel_dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Load the DICOMs for this scan.\n",
    "dr = dm.DicomReader(group_by=[\"EchoNumbers\", \"SeriesDescription\"], verbose=True, num_workers=4)\n",
    "volumes = dr.load(scan[\"dicom_dir\"])\n",
    "\n",
    "# Filter out unnecessary dicoms.\n",
    "volumes = [v for v in volumes if \"T2\" not in v.get_metadata(\"SeriesDescription\")]\n",
    "assert len(volumes) == 2\n",
    "echo1, echo2 = tuple(sorted(volumes, key=lambda x: x.get_metadata(\"EchoNumbers\")))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "ca289c7160af4c159ad7fd411d912ef7",
      "015d4fbb69af450585e5e2fa60412047",
      "ea4b2048a1ec44e099bb923bad633b9a",
      "51a3a30ad84f49918cbda78bc33ccbd4",
      "842b85d281ce4c239af9436d6e57958e",
      "550602d30f904c8f8123b8366342950e",
      "1717cc1ff8934b0b9020c7a0cd2b595b",
      "d0e0ecc7cc104ba1a7d2918a3f585b41",
      "67aba4eecf5141e0bbb4b0da63ff8c51",
      "2ff53edd8c7a453395cdad88414d1212",
      "699878da64014757921c9ef70119e50e"
     ]
    },
    "id": "Mx9ngDEOx-sm",
    "outputId": "55581b63-ec42-4305-a645-612edfd382b7"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot reconstructed image\n",
    "seg_colorized = label2rgb(segmentation, bg_label=0)\n",
    "\n",
    "_ = plot_images(\n",
    "  [echo1, echo2],\n",
    "  titles=[\"Echo 1\", \"Echo 2\"],\n",
    "  overlay=seg_colorized,\n",
    "  opacity=0.4,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## üêò Model Zoo\n",
    "Interested in running a pre-trained model on your data? We got you!\n",
    "\n",
    "We maintain a model zoo of pre-trained models that have been trained on the SKM-TEA dataset for different tasks. You can find a list of these models on [GitHub](https://github.com/StanfordMIMI/skm-tea).\n",
    "\n",
    "And loading the model is as easy as 123! Just use the `skm_tea.get_model_from_zoo`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load a scan from the test dataset.\n",
    "dataset_dicts = DatasetCatalog.get(\"skmtea_v1_test\")\n",
    "scan = dataset_dicts[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Reconstruction\n",
    "Let's use a pretrained [unrolled network](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7664163/) to reconstruct 6x accelerated qDESS scans.\n",
    "\n",
    "The reconstruction model was trained to reconstruction axial ($k_y \\times k_z$) slices for the first echo. You can find other pretrained reconstruction models [here](https://github.com/StanfordMIMI/skm-tea/blob/main/MODEL_ZOO.md#reconstruction-baselines).\n",
    "\n",
    "*Aside*: When reporting results on the SKM-TEA dataset, please use the masks provided with the dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Simulate 6x undersampled data\n",
    "sl = 256\n",
    "\n",
    "with h5py.File(scan[\"recon_file\"], \"r\") as f:\n",
    "    kspace = torch.as_tensor(f[\"kspace\"][sl, :, :, :, :]).unsqueeze(0)\n",
    "    maps = torch.as_tensor(f[\"maps\"][sl, :, :, :, :]).unsqueeze(0)\n",
    "    mask = torch.as_tensor(f[\"masks/poisson_6.0x\"][()]).unsqueeze(0)  # TODO: Fix\n",
    "    img_gt = torch.as_tensor(f[\"target\"][sl, :, :, :, :]).unsqueeze(0)\n",
    "mask = oF.zero_pad(mask, kspace.shape[1:3])\n",
    "\n",
    "us_kspace = kspace * mask.unsqueeze(-1).unsqueeze(-1).type(kspace.dtype)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Fetch the model with pretrained weights.\n",
    "model = st.get_model_from_zoo(\n",
    "    cfg_or_file=\"https://huggingface.co/arjundd/skm-tea-models/raw/main/neurips2021/recon-models/6x/Unrolled_E1/config.yaml\",\n",
    "    weights_path=\"https://huggingface.co/arjundd/skm-tea-models/resolve/main/neurips2021/recon-models/6x/Unrolled_E1/model.ckpt\",\n",
    ").to(DEVICE).eval()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "echo = 0  # the 1st echo\n",
    "echo1_kspace = us_kspace[..., echo, :]\n",
    "with torch.no_grad():\n",
    "    pred = model({\"kspace\": echo1_kspace, \"maps\": maps})[\"pred\"].cpu()\n",
    "echo1_recon = pred"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# For visualization purposes, we scale the ground truth and reconstructions\n",
    "# to get rid of very bright outliers.\n",
    "gt_abs = get_scaled_image(img_gt[..., 0, :].abs(), 0.9999, clip=True)\n",
    "recon_abs = get_scaled_image(echo1_recon.abs(), 0.9999, clip=True)\n",
    "err = torch.abs(gt_abs - recon_abs)\n",
    "\n",
    "plot_images(\n",
    "    [gt_abs, recon_abs, err * 4],\n",
    "    processor=lambda x: x.abs().squeeze(),\n",
    "    titles=[\"Ground truth\", \"Recon\", \"Error (4x)\"],\n",
    "    hsize=5, wsize=2.3\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Segmentation\n",
    "Let's perform segmentation on the DICOM track dataset using a pretrained [U-Net](https://arxiv.org/abs/1505.04597).\n",
    "\n",
    "The segmentation model was trained to segment sagittal slices for the first echo. You can find other pretrained segmentation models [here](https://github.com/StanfordMIMI/skm-tea/blob/main/MODEL_ZOO.md#segmentation-baselines).\n",
    "\n",
    "**Note:** The volume has to first be normalized to have zero-mean and unit standard deviation. In the near future, this will automatically be done."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = st.get_model_from_zoo(\n",
    "    cfg_or_file=\"https://huggingface.co/arjundd/skm-tea-models/raw/main/neurips2021/segmentation-models/U-Net_E1/config.yaml\",\n",
    "    weights_path=\"https://huggingface.co/arjundd/skm-tea-models/resolve/main/neurips2021/segmentation-models/U-Net_E1/model.ckpt\",\n",
    ").eval()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from meddlr.data.data_utils import collect_mask\n",
    "sl = 88  # the slice to segment\n",
    "\n",
    "# DICOM data + segmentation\n",
    "image_file = scan[\"image_file\"]\n",
    "with h5py.File(image_file, \"r\") as f:\n",
    "    echo1 = f[\"echo1\"][()]  # Shape: (x, y, z)\n",
    "    segmentation = f[\"seg\"][()]  # Shape: (x, y, z, #classes)\n",
    "\n",
    "echo1 = torch.as_tensor(echo1).unsqueeze(0).unsqueeze(0).float()  # Shape: (B, C, H, W)\n",
    "\n",
    "# Ground truth segmentation\n",
    "# Medial/lateral components are aggregated into the same category.\n",
    "# 0 - patellar cartilage, 1 - femoral cartilage\n",
    "# 2/3 - medial/lateral tibial cartilage, 4/5 - medial/lateral meniscus\n",
    "gt_seg_sl = segmentation[..., sl, :]\n",
    "gt_seg_sl = collect_mask(gt_seg_sl, (0, 1, (2, 3), (4, 5)), out_channel_first=False)\n",
    "gt_seg_sl = oF.one_hot_to_categorical(gt_seg_sl, channel_dim=-1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Normalize volume and run model.\n",
    "echo1 = (echo1 - echo1.mean()) / echo1.std()\n",
    "echo1_sl = echo1[..., sl]\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model({\"image\": echo1_sl})[\"sem_seg_logits\"]\n",
    "\n",
    "prediction = oF.pred_to_categorical(logits, activation='sigmoid').squeeze(0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "_, axs = plt.subplots(1, 3, figsize=(10,5))\n",
    "for idx, (data, title) in enumerate([\n",
    "  (echo1_sl.squeeze(), \"Input\"), (prediction, \"Prediction\"), (gt_seg_sl, \"Ground truth\")\n",
    "]):\n",
    "    ax = axs[idx]\n",
    "    ax.imshow(data.squeeze(), cmap=\"gray\" if idx == 0 else None)\n",
    "    ax.set_title(title, fontsize=20)\n",
    "    ax.axis(\"off\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## üìä qMRI Evaluation\n",
    "SKM-TEA introduces a new family of metrics based on quantitative MRI (qMRI) endpoints. In this section, we will explore the utility of these metrics and how to use them to benchmark your models.\n",
    "\n",
    "Specifically, we will consider a qMRI knee analysis pipeline that uses qDESS reconstructions to analytically estimate $T_2$ maps and uses automated segmentations to get region-specific $T_2$ values.\n",
    "\n",
    "As a proof-of-concept, let's dive into how we can use qMRI endpoints to evaluate a segmentation model based on regional $T_2$ accuracy. We will evaluate the same pretrained U-Net model from the [Model Zoo section](https://colab.research.google.com/drive/1PluqK77pobD5dXE7zzBLEAeBgaaeGKXa?authuser=1#scrollTo=zOBE1rQAMl6Y&line=6&uniqifier=1).\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1LhO6GlVWtyOZ5AmBFrgdECakqtPOfH2k'>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from skm_tea.metrics import QuantitativeKneeMRI\n",
    "from meddlr.data.data_utils import collect_mask\n",
    "\n",
    "from dosma.scan_sequences import QDess"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load a scan and corresponding metadata.\n",
    "dataset_dicts = DatasetCatalog.get(\"skmtea_v1_test\")\n",
    "scan = dataset_dicts[0]\n",
    "\n",
    "metadata: pd.DataFrame = MetadataCatalog.get(\"skmtea_v1_test\").scan_metadata\n",
    "metadata = metadata[metadata[\"MTR_ID\"] == scan[\"scan_id\"]]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Load the ground truth segmentation.\n",
    "seg_gt = dm.read(scan[\"dicom_mask_file\"])\n",
    "arr = oF.categorical_to_one_hot(seg_gt.A, channel_dim=-1)\n",
    "arr = collect_mask(arr, (0, 1, (2,3), (4,5)), out_channel_first=False)\n",
    "\n",
    "seg_gt = dm.MedicalVolume(arr, affine=seg_gt.affine)"
   ],
   "metadata": {
    "id": "q68mUZqFz7Qz"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Run the model"
   ],
   "metadata": {
    "id": "qS22VFNe8Asz"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def run_segmentation(\n",
    "    mv: dm.MedicalVolume, model: nn.Module, normalize=True,\n",
    "    batch_size: int = 4, pbar: bool = True\n",
    "):\n",
    "  \"\"\"Runs a segmentation model on the qDESS volume.\n",
    "\n",
    "  The model should be trained to segment sagittal slices.\n",
    "\n",
    "  Args:\n",
    "    x (dm.MedicalVolume): A 3D magnitude image (single echo).\n",
    "    model (nn.Module): The segmentation model to run.\n",
    "    normalize (bool): Whether to perform zero-mean, unit-std normalization.\n",
    "    batch_size (int): The batch size for performing segmentation.\n",
    "    pbar (bool): Whether to display progress bar.\n",
    "\n",
    "  Returns:\n",
    "    dm.MedicalVolume: The one-hot predictions from the segmentation model\n",
    "      where last dimension/axis is the channel dimension.\n",
    "  \"\"\"\n",
    "  mv_ornt = mv.orientation\n",
    "  mv = mv.reformat((\"LR\", \"SI\", \"AP\"))\n",
    "  affine = mv.affine.copy()\n",
    "\n",
    "  x = mv.to_torch().type(torch.float32)\n",
    "  if normalize:\n",
    "    x = (x - x.mean()) / x.std()\n",
    "\n",
    "  x_chunks = torch.split(x, batch_size, dim = 0)\n",
    "\n",
    "  logits = []\n",
    "  for chunk in tqdm(torch.split(x, batch_size, dim=0), disable=not pbar):\n",
    "    chunk = chunk.unsqueeze(1)  # add a channel dimension\n",
    "    out = model({\"image\": chunk})\n",
    "    logits.append(out[\"sem_seg_logits\"])\n",
    "\n",
    "\n",
    "  logits = torch.concat(logits, dim=0)\n",
    "  prediction = torch.sigmoid(logits).permute(0, 2, 3, 1)  # make channels last\n",
    "\n",
    "  out = dm.MedicalVolume.from_torch(prediction, affine).reformat(mv_ornt)\n",
    "  return out"
   ],
   "metadata": {
    "id": "9_AnZ_0qNiRr"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "model = st.get_model_from_zoo(\n",
    "    cfg_or_file=\"https://huggingface.co/arjundd/skm-tea-models/raw/main/neurips2021/segmentation-models/U-Net_E1/config.yaml\",\n",
    "    weights_path=\"https://huggingface.co/arjundd/skm-tea-models/resolve/main/neurips2021/segmentation-models/U-Net_E1/model.ckpt\",\n",
    ").to(DEVICE).eval()"
   ],
   "metadata": {
    "id": "CrnRYP5tEGGN"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "with torch.no_grad():\n",
    "  seg_pred = run_segmentation(echo1.to(DEVICE), model, batch_size=4)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SBoA-EfcEX2N",
    "outputId": "c5f873c1-9ebb-485b-97e6-6f9626b2f7d3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Computing $T_2$ Maps\n",
    "\n",
    "Computing  $T_2$  maps from qDESS can be done analytically, which is much faster than traditional fitting. To do so, we require a few scan parameters as well as rough estimates for  $T_1$  of tissues. Scan parameters can be found in the DICOM files or the metadata file shipped with the dataset.\n",
    "\n",
    "An open-source implementation of the analytical fit is available in dosma. To ensure standardization, dosma should be used to perform all qMRI evaluation in SKM-TEA.\n",
    "\n",
    "IMPORTANT: Do not use the scanner-generated $T_2$ maps (available in the dicom folder) for analysis. These should be used for visualization purposes only."
   ],
   "metadata": {
    "id": "E7cTiTk8Hqps"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As mentioned above, we need rough estimates for $T_1$ of tissues for the analytical $T_2$ estimation. From [literature](), we know that femoral, tibial, and patellar (articular) cartilage has a $T_1$ of approximately 1.2sec and meniscus has a $T_1$ of ~1sec.\n",
    "\n",
    "We can use the segmentation to fill in the expected $T_1$ values. Note, we will have 2 $T_1$ maps -- one from the ground truth segmentation (`t1_gt`), and one from the predicted segmentation (`t1_pred`)."
   ],
   "metadata": {
    "id": "_MMDFtcY1Q0R"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# For reconstruction, this would be based on reconstructions for E1/E2.\n",
    "# Estimated T1 values are 1.2s for cartilage and 1s for meniscus\n",
    "def get_t1(seg: dm.MedicalVolume):\n",
    "  \"\"\"Build T1 maps based on the segmentation.\n",
    "\n",
    "  `seg[..., 3]` should correspond to the meniscus segmentation map.\n",
    "\n",
    "  Args:\n",
    "    seg (dm.MedicalVolume): A one-hot encoded segmentation mask, where the\n",
    "      last dimension is the channel dimension.\n",
    "\n",
    "  Returns:\n",
    "    dm.MedicalVolume: The estimated T1 map (in milliseconds).\n",
    "  \"\"\"\n",
    "  t1 = dm.MedicalVolume(np.ones(seg.shape[:3]) * 1200, seg.affine).to(seg.device)\n",
    "  t1[seg.A[..., 3].astype(bool)] = 1000\n",
    "  return t1\n",
    "\n",
    "t1_gt = get_t1(seg_gt)\n",
    "t1_pred = get_t1(seg_pred)"
   ],
   "metadata": {
    "id": "JfMeAPjrrAwu"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y6k1z3LtMl6Z",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "5c302053-03fc-4087-fcef-d4697dc9f8bc"
   },
   "outputs": [],
   "source": [
    "def compute_t2_map(t1: dm.MedicalVolume):\n",
    "  qdess = QDess([echo1, echo2]).to(t1.device)\n",
    "  t2map = qdess.generate_t2_map(\n",
    "      suppress_fat=True,\n",
    "      suppress_fluid=True,\n",
    "      gl_area=float(metadata[\"SpoilerGradientArea\"]),\n",
    "      tg=float(metadata[\"SpoilerGradientTime\"]),\n",
    "      tr=float(metadata[\"RepetitionTime\"]),\n",
    "      te=float(metadata[\"EchoTime1\"]),\n",
    "      alpha=float(metadata[\"FlipAngle\"]),\n",
    "      t1=t1,\n",
    "      nan_bounds=(0, 100),\n",
    "      nan_to_num=True,\n",
    "  )\n",
    "  return t2map.volumetric_map\n",
    "\n",
    "t2_gt = compute_t2_map(t1_gt)\n",
    "t2_pred = compute_t2_map(t1_pred)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "sl = 60  # Sagittal slice to plot\n",
    "\n",
    "plot_images(\n",
    "  [t2_gt, t2_pred],\n",
    "  processor=lambda x: x.cpu().A[..., sl],\n",
    "  titles=[\"T2 (Ground Truth)\", \"T2 (Pred)\"],\n",
    "  cmap=\"viridis\", show_cbar=True,\n",
    ")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 352
    },
    "id": "4k3lVDg52trB",
    "outputId": "4d6622c5-3dc1-4257-b050-136c477e347e"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The `QuantitativeKneeMRI` metric\n",
    "\n",
    "`QuantitativeKneeMRI` metrics simplifies computing and tracing qMRI related metrics for key knee anatomical structures.\n",
    "\n",
    "We will use `qmri_gt` and `qmri_pred` to track regional $T_2$ measures extracted from the ground truth and predicted segmentations, respectively. These regions will correspond to the four segmented tissues: patellar cartilage (`pc`), femoral cartilage (`fc`), tibial cartilage (`tc`), and meniscus (`men`)\n",
    "\n",
    "We can also choose to compute qMRI measures for anatomically relevant subregions in the these tissues. To do this, set `use_subregions=True`. Note the subregion division can be time intensive.\n",
    "\n",
    "**Note**: The metric is stateful. This means each time the metric is called, it stores the results. Use `.reset()` to reset the metric and clear all stored results.\n",
    "\n",
    "*Aside*: These metrics are automatically computed under the hood with the [`skm_tea.evaluation.SkmTeaEvaluator`](https://github.com/StanfordMIMI/skm-tea/blob/main/skm_tea/evaluation/qdess_evaluation.py)."
   ],
   "metadata": {
    "id": "zCn3xtvfuBtt"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "TrdR9XsOMl6a"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'QuantitativeKneeMRI' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 5\u001B[0m\n\u001B[1;32m      2\u001B[0m use_cpu \u001B[38;5;241m=\u001B[39m use_subregions  \u001B[38;5;66;03m# computing subregions is currently limited to the CPU\u001B[39;00m\n\u001B[1;32m      3\u001B[0m tissues \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpc\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfc\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtc\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmen\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m----> 5\u001B[0m qmri_gt \u001B[38;5;241m=\u001B[39m \u001B[43mQuantitativeKneeMRI\u001B[49m(channel_names\u001B[38;5;241m=\u001B[39mtissues, subregions\u001B[38;5;241m=\u001B[39muse_subregions, use_cpu\u001B[38;5;241m=\u001B[39muse_cpu)\n\u001B[1;32m      6\u001B[0m qmri_pred \u001B[38;5;241m=\u001B[39m QuantitativeKneeMRI(channel_names\u001B[38;5;241m=\u001B[39mtissues, subregions\u001B[38;5;241m=\u001B[39muse_subregions, use_cpu\u001B[38;5;241m=\u001B[39muse_cpu)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'QuantitativeKneeMRI' is not defined"
     ]
    }
   ],
   "source": [
    "use_subregions = False\n",
    "use_cpu = use_subregions  # computing subregions is currently limited to the CPU\n",
    "tissues = [\"pc\", \"fc\", \"tc\", \"men\"]\n",
    "\n",
    "qmri_gt = QuantitativeKneeMRI(channel_names=tissues, subregions=use_subregions, use_cpu=use_cpu)\n",
    "qmri_pred = QuantitativeKneeMRI(channel_names=tissues, subregions=use_subregions, use_cpu=use_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Tnrul8RMl6a",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "50572ff7-b3bb-4d59-9fbd-3c68cd340d9a"
   },
   "outputs": [],
   "source": [
    "# Reset the metrics\n",
    "qmri_gt.reset()\n",
    "qmri_pred.reset()\n",
    "\n",
    "# Compute regional qMRI estimates using ground truth and predicted segmentations.\n",
    "qmri_gt(ids=[\"MTR_005\"], quantitative_map=[t2_gt], sem_seg=[seg_gt], medial_direction=metadata[\"MedialDirection\"])\n",
    "qmri_pred(ids=[\"MTR_005\"], quantitative_map=[t2_pred], sem_seg=[seg_pred], medial_direction=metadata[\"MedialDirection\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kUwnBDOqMl6a",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 99
    },
    "outputId": "7209be8b-5d89-43a1-fb48-35103eba8fe9"
   },
   "outputs": [],
   "source": [
    "print(\"Ground Truth Regional T2 Estimates:\")\n",
    "display(qmri_gt.to_pandas())"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"Predicted Regional T2 Estimates:\")\n",
    "display(qmri_pred.to_pandas())"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 99
    },
    "id": "d5dqjITe5IkJ",
    "outputId": "03b07adc-1123-410c-8d90-0fc5c311890f"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "2eb31e4132ee4926db264fe71a873573f5351ed39181c53ae251bffe4e1faa2d"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "ca289c7160af4c159ad7fd411d912ef7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_015d4fbb69af450585e5e2fa60412047",
       "IPY_MODEL_ea4b2048a1ec44e099bb923bad633b9a",
       "IPY_MODEL_51a3a30ad84f49918cbda78bc33ccbd4"
      ],
      "layout": "IPY_MODEL_842b85d281ce4c239af9436d6e57958e"
     }
    },
    "015d4fbb69af450585e5e2fa60412047": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_550602d30f904c8f8123b8366342950e",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_1717cc1ff8934b0b9020c7a0cd2b595b",
      "value": "100%"
     }
    },
    "ea4b2048a1ec44e099bb923bad633b9a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d0e0ecc7cc104ba1a7d2918a3f585b41",
      "max": 480,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_67aba4eecf5141e0bbb4b0da63ff8c51",
      "value": 480
     }
    },
    "51a3a30ad84f49918cbda78bc33ccbd4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2ff53edd8c7a453395cdad88414d1212",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_699878da64014757921c9ef70119e50e",
      "value": " 480/480 [00:02&lt;00:00, 228.84it/s]"
     }
    },
    "842b85d281ce4c239af9436d6e57958e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "550602d30f904c8f8123b8366342950e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1717cc1ff8934b0b9020c7a0cd2b595b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d0e0ecc7cc104ba1a7d2918a3f585b41": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "67aba4eecf5141e0bbb4b0da63ff8c51": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2ff53edd8c7a453395cdad88414d1212": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "699878da64014757921c9ef70119e50e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
